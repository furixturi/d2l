{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用 PyTorch 自动求导\n",
    "\n",
    "## 正向传播和反向传播\n",
    "\n",
    "自动求导计算一个函数在指定值上时的导数。自动求导先会生成一个无环计算图，可以是现实构造（Tensorflow/Theano/MXNet）也可以是隐式构造（PyTorch/MXNet）。\n",
    "\n",
    "根据链式法则，求导可以看成是中间部分各自求导再相乘。\n",
    "\n",
    "自动求导有两种模式：正向传播，反向传播(back propogation)\n",
    "\n",
    "正向传播从最内部包含 x 作为 input 的部分开始向外计算，反向传播则从最外层向内计算，使用正向计算函数时已经算好的中间值。机器学习中一般用反向传播。\n",
    "\n",
    "### 反向传播\n",
    "跟踪整改计算图，填充关于每个参数的偏导数。\n",
    "\n",
    "#### 流程\n",
    "1. 构造计算图\n",
    "2. 前向执行计算图，存储中间结果\n",
    "3. 反向执行图，计算导数\n",
    "   1. 去除不需要的枝\n",
    "   \n",
    "#### 反向传播总结\n",
    "计算图的操作子个数为 n。\n",
    "- 计算复杂度：O(n)，与正向传播一样\n",
    "- 内存复杂度：O(n)，因为需要存储\n",
    "  - 正向传播为 O(1)，但每步都需要扫一遍全部计算图\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自动求导的 PyTorch 实现\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 例：\n",
    "\n",
    "对函数 y = 2x<sup>T</sup>x 关于列向量 x 求导。一个标量函数对向量求梯度，得到的是一个与之形状相同的向量。\n",
    "\n",
    "根据点积求导公式\n",
    "\n",
    "y = <u, v>, dy/d<b>x</b> = u<sup>T</sup> * (dv/dx) + v<sup>T</sup> * (du/dx)\n",
    "\n",
    "梯度应为 4x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.arange(4.0)\n",
    "x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算 y 关于 x 梯度前，需要有个地方存储梯度。可以用 `requires_grad` 参数在创建向量时准备（ `torch.arange(4.0, requires_grad=True)`），也可以在之后用 `x.requires_grad_(True)` 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.requires_grad_(True)\n",
    "x.grad # 默认是 None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算 y，是个标量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(28., grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = 2 * torch.dot(x, x)\n",
    "y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "反向传播，这步会自动计算 y 关于 x 向量的每个分量的梯度。于是我们就可以打印梯度 `x.grad`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  4.,  8., 12.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad == 4 * x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch 默认会累积梯度。当要计算 x 的另一个函数的梯度，先要清除 x 现在的梯度 `x.grad.zero_()`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算另一个关于 x 的函数的梯度\n",
    "y = x.sum()\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 2., 2., 2.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 再运行一次反向传播，可以看到梯度被累积了\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 非标量 y\n",
    "\n",
    "当 y 不是标量时，向量 y 关于向量 x 的导数会是一个矩阵。更高阶时则会是一个高阶张量。\n",
    "此时计算 `backward()` 时要传入一个 `gradient` 参数。\n",
    "\n",
    "机器学习中我们通常不是想计算高阶微分张量，而是想要求一个 batch 的训练样本中各个部分的损失函数的偏导数的和，所以可以传入一个全是 1 的向量作为 gradient 参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 2., 4., 6.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y = x * x\n",
    "y.backward(torch.ones(len(x)))\n",
    "# same as y.sum().backward()\n",
    "x.grad"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 分离计算\n",
    "\n",
    "用 `detach()` 将一部分计算分离作为常量看待，这样反向传播计算梯度时就不会考虑计算图的这个计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([0., 1., 2., 3.], requires_grad=True)\n",
      "u: tensor([0., 1., 4., 9.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 4., 9.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y = x * x\n",
    "\n",
    "# u 成为一个储存 y 计算结果的常量向量\n",
    "u = y.detach()\n",
    "print('x:', x)\n",
    "print('u:', u)\n",
    "\n",
    "# z 是用一个常量向量 [0,1,4,9] 乘以 x\n",
    "z = u * x\n",
    "z.sum().backward()\n",
    "x.grad\n",
    "# x 的梯度与 u 相同"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用了Python控制流的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This eseentially returns c * x, \n",
    "# c is determined with complex while, if else flow\n",
    "def f(x):\n",
    "    b = torch.randn(size=())\n",
    "    while b.norm() < 1000:\n",
    "        b = b * 2\n",
    "    if b.sum() > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    return c * x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When size is set to an empty tuple (), as in torch.randn(size=()), it indicates that a tensor with a single element should be created. In other words, this creates a scalar tensor with a random value sampled from a standard normal distribution (i.e., mean 0 and standard deviation 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: tensor(-0.0484, requires_grad=True)\n",
      "d: tensor(-64.9551, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(1341.9515), tensor(1341.9515, grad_fn=<DivBackward0>))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(size=(), \\\n",
    "    requires_grad=True)\n",
    "print('a:', a)\n",
    "\n",
    "# 用了 Python 控制流的复杂的计算图\n",
    "d = f(a)\n",
    "print('d:', d)\n",
    "\n",
    "d.backward()\n",
    "a.grad, d/a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
