{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra Review"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaler 标量, Vector 向量，Matrix 矩阵，Tensor 张量"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaler 标量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(5.), tensor(1.), tensor(6.), tensor(1.5000), tensor(9.))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scaler\n",
    "x = torch.tensor(3.0)\n",
    "y = torch.tensor(2.0)\n",
    "\n",
    "x + y, x - y, x * y, x / y, x ** y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector 向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2, 3]), tensor(3))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vector\n",
    "x = torch.arange(4)\n",
    "x, x[3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "大量文献认为列向量是向量的默认方向，所以 `x` 是\n",
    "```\n",
    "0\n",
    "1\n",
    "2\n",
    "3\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 向量的长度、维度和形状"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "向量的长度跟维度一样，表示包含了几个元素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 用 array 的 len() 函数访问\n",
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 用 PyTorch tensor 的 shape attribute 访问\n",
    "x.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix 矩阵"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "矩阵将向量从一维推广到二维。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11],\n",
       "        [12, 13, 14, 15],\n",
       "        [16, 17, 18, 19]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建一个5行4列的矩阵\n",
    "A = torch.arange(20).reshape(5,4)\n",
    "A"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "转置 (transpose) 一个矩阵\n",
    "```\n",
    "A[i][j] = A.T[j][i]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  4,  8, 12, 16],\n",
       "        [ 1,  5,  9, 13, 17],\n",
       "        [ 2,  6, 10, 14, 18],\n",
       "        [ 3,  7, 11, 15, 19]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 转置它为4行5列的矩阵\n",
    "A.T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特殊矩阵：symmetric matrix\n",
    "A = A.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 2, 3],\n",
       "         [2, 0, 4],\n",
       "         [3, 4, 5]]),\n",
       " tensor([[True, True, True],\n",
       "         [True, True, True],\n",
       "         [True, True, True]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = torch.tensor([[1, 2, 3], [2, 0, 4], [3, 4, 5]])\n",
    "B, B == B.T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor 张量"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "矩阵的推广， n维。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11]],\n",
       "\n",
       "        [[12, 13, 14, 15],\n",
       "         [16, 17, 18, 19],\n",
       "         [20, 21, 22, 23]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(24).reshape(2,3,4)\n",
    "X"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 运算"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 元素级运算\n",
    "\n",
    "Element-wise calculation will not change the shape of a scaler/vector/matrix/tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19.]]),\n",
       " tensor([[ 0.,  2.,  4.,  6.],\n",
       "         [ 8., 10., 12., 14.],\n",
       "         [16., 18., 20., 22.],\n",
       "         [24., 26., 28., 30.],\n",
       "         [32., 34., 36., 38.]]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(20, dtype=torch.float32). \\\n",
    "                reshape(5,4)\n",
    "B = A.clone()\n",
    "A, A+B"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Element-wise multiplication of two matrices is claeed Hadamard product (哈达马积) <math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n",
    "  <mo>&#x2299;</mo>\n",
    "</math>\n",
    "\n",
    "\\begin{split}\\mathbf{A} \\odot \\mathbf{B} =\n",
    "\\begin{bmatrix}\n",
    "    a_{11}  b_{11} & a_{12}  b_{12} & \\dots  & a_{1n}  b_{1n} \\\\\n",
    "    a_{21}  b_{21} & a_{22}  b_{22} & \\dots  & a_{2n}  b_{2n} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    a_{m1}  b_{m1} & a_{m2}  b_{m2} & \\dots  & a_{mn}  b_{mn}\n",
    "\\end{bmatrix}.\\end{split}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.,   1.,   4.,   9.],\n",
       "        [ 16.,  25.,  36.,  49.],\n",
       "        [ 64.,  81., 100., 121.],\n",
       "        [144., 169., 196., 225.],\n",
       "        [256., 289., 324., 361.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A * B"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当一个标量与一个张量进行运算，标量将于张量的每一个元素进行运算，运算结果将是一个与张量形状一致的张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 2,  3,  4,  5],\n",
       "          [ 6,  7,  8,  9],\n",
       "          [10, 11, 12, 13]],\n",
       " \n",
       "         [[14, 15, 16, 17],\n",
       "          [18, 19, 20, 21],\n",
       "          [22, 23, 24, 25]]]),\n",
       " tensor([[[ 0,  2,  4,  6],\n",
       "          [ 8, 10, 12, 14],\n",
       "          [16, 18, 20, 22]],\n",
       " \n",
       "         [[24, 26, 28, 30],\n",
       "          [32, 34, 36, 38],\n",
       "          [40, 42, 44, 46]]]),\n",
       " torch.Size([2, 3, 4]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 2\n",
    "X = torch.arange(24). \\\n",
    "    reshape(2,3,4)\n",
    "a + X, a * X, (a * X).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 求和跟平均值\n",
    "\n",
    "对张量沿任意轴进行求和。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2., 3.]), tensor(6.))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sum a 1-d vector, the result is a scaler (0-d)\n",
    "x = torch.arange(4, dtype=torch.float32)\n",
    "x, x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      " tensor([[[1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.]]])\n",
      "A.sum(): tensor(60.) \n",
      " A.sum().shape: torch.Size([])\n",
      "A.sum(axis=0):\n",
      " tensor([[3., 3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3., 3.]]) \n",
      " A.sum(axis=0).shape: torch.Size([4, 5])\n",
      "A.sum(axis=[0,2]):\n",
      " tensor([15., 15., 15., 15.]) \n",
      " A.sum(axis=[0,2]).shape: torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "A = torch.ones(3,4,5)\n",
    "print('A:\\n', A)\n",
    "\n",
    "# sum every axis\n",
    "A_sum = A.sum()\n",
    "print('A.sum():', A_sum, '\\n', \\\n",
    "    'A.sum().shape:', A_sum.shape)\n",
    "\n",
    "# sum along axis 0\n",
    "A_sum_0 = A.sum(axis=0)\n",
    "print('A.sum(axis=0):\\n', A_sum_0,'\\n', \\\n",
    "    'A.sum(axis=0).shape:', A_sum_0.shape)\n",
    "\n",
    "# sum along two axes\n",
    "A_sum_0_2 = A.sum(axis=[0,2])\n",
    "print('A.sum(axis=[0,2]):\\n', A_sum_0_2, '\\n', \\\n",
    "    'A.sum(axis=[0,2]).shape:', A_sum_0_2.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "求均值也可以沿轴降维。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: tensor(9.5000) tensor(9.5000)\n",
      "mean along axis 0: tensor([ 8.,  9., 10., 11.]) tensor([ 8.,  9., 10., 11.])\n"
     ]
    }
   ],
   "source": [
    "A = torch.arange(20, dtype=torch.float32). \\\n",
    "    reshape(5,4)\n",
    "# the mean of all elements\n",
    "A_mean = A.mean()\n",
    "print('mean:', A_mean, A.sum() / A.numel())\n",
    "\n",
    "# the mean along \n",
    "A_mean_0 = A.mean(axis=0)\n",
    "print('mean along axis 0:', A_mean_0, \\\n",
    "    A.sum(axis=0)/A.shape[0]) # shape has axes, too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "非降维求和。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 6., 22., 38., 54., 70.]),\n",
       " tensor([[ 6.],\n",
       "         [22.],\n",
       "         [38.],\n",
       "         [54.],\n",
       "         [70.]]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_A_1 = A.sum(axis=1)\n",
    "sum_A_1_keepdim = A.sum(axis=1, keepdim=True)\n",
    "sum_A_1, sum_A_1_keepdim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19.]]),\n",
       " tensor([[ 0.,  1.,  3.,  6.],\n",
       "         [ 4.,  9., 15., 22.],\n",
       "         [ 8., 17., 27., 38.],\n",
       "         [12., 25., 39., 54.],\n",
       "         [16., 33., 51., 70.]]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 按某一个轴求每个元素位置的累积和，而保留每个维度\n",
    "A, A.cumsum(axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 乘法"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 点积 dot product\n",
    "\n",
    "两个向量的点积\n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n",
    "  <msup>\n",
    "    <mrow data-mjx-texclass=\"ORD\">\n",
    "      <mi mathvariant=\"bold\">x</mi>\n",
    "    </mrow>\n",
    "    <mi mathvariant=\"normal\">&#x22A4;</mi>\n",
    "  </msup>\n",
    "  <mrow data-mjx-texclass=\"ORD\">\n",
    "    <mi mathvariant=\"bold\">y</mi>\n",
    "  </mrow>\n",
    "</math> 或\n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n",
    "  <mo fence=\"false\" stretchy=\"false\">&#x27E8;</mo>\n",
    "  <mrow data-mjx-texclass=\"ORD\">\n",
    "    <mi mathvariant=\"bold\">x</mi>\n",
    "  </mrow>\n",
    "  <mo>,</mo>\n",
    "  <mrow data-mjx-texclass=\"ORD\">\n",
    "    <mi mathvariant=\"bold\">y</mi>\n",
    "  </mrow>\n",
    "  <mo fence=\"false\" stretchy=\"false\">&#x27E9;</mo>\n",
    "</math> 是相同位置的按元素乘积的和。\n",
    "\n",
    "向量点积可以表示一组值的加权和。例如，w 是一个权重向量（各个值都在0到1之间且和为1），则 \\<x, w\\> 表示向量 x 的加权和。\n",
    "\n",
    "几何上，当两个向量规范化得到单位长度后，点积表示它们夹角的余弦。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3.]) tensor([4., 5., 6., 7.])\n",
      "tensor(38.)\n",
      "tensor([ 0.,  5., 12., 21.]) tensor(38.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([0., 1, 2, 3])\n",
    "y = torch.tensor([4., 5, 6, 7])\n",
    "\n",
    "print(x,y)\n",
    "\n",
    "# dot product\n",
    "print(torch.dot(x,y)) \n",
    "\n",
    "# is the same as sum of \n",
    "# element-wise products\n",
    "print(x*y, torch.sum(x*y))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 矩阵-向量积 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "m x n 矩阵 A 跟 n 维列向量 x 的积是一个长度为 m 的列向量 Ax，其每个元素为 A 的每一行向量跟 x 这个列向量的点积。我们可以用矩阵-向量乘法来转换向量的维度 （在这个例子中，从 n 维到 m 维）。\n",
    "\n",
    "PyTorch中我们用 `torch.mv(matrix, vector)` 来求矩阵-向量积。\n",
    "\n",
    "如果要求向量-矩阵积，可以用 `torch.matmul(vector, matrix)` 。不管用哪个，如果维度不能匹配，PyTorch 都会 raise a `RuntimeError` with the message `size mismatch`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 1.],\n",
       "         [2., 3.],\n",
       "         [4., 5.],\n",
       "         [6., 7.]]),\n",
       " tensor([0., 1.]),\n",
       " tensor([[0., 1.],\n",
       "         [0., 3.],\n",
       "         [0., 5.],\n",
       "         [0., 7.]]),\n",
       " tensor([1., 3., 5., 7.]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(8, dtype=torch.float32).reshape(4,2)\n",
    "x = torch.arange(2, dtype=torch.float32)\n",
    "A, x, A*x,torch.mv(A, x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 矩阵-矩阵乘法"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "m x k 矩阵 A 跟 k x n 矩阵 B 相乘得到一个 m x n 矩阵 C。C 的每个元素为相应的 A 的行向量跟 B 的列向量的点积。这也可以看做是执行 m 次矩阵-向量积。\n",
    "\n",
    "Pytorch中我们用 `torch.mm(matrix1, matrix2)` 来计算矩阵-矩阵乘法。注意这跟 Hadamard 积（每个元素的乘积，PyTorch 中的 `A * B` ）不同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 1.],\n",
       "         [2., 3.],\n",
       "         [4., 5.],\n",
       "         [6., 7.]]),\n",
       " tensor([[1., 1., 1.],\n",
       "         [1., 1., 1.]]),\n",
       " tensor([[ 1.,  1.,  1.],\n",
       "         [ 5.,  5.,  5.],\n",
       "         [ 9.,  9.,  9.],\n",
       "         [13., 13., 13.]]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = torch.ones((2,3), dtype=torch.float32)\n",
    "A, B, torch.mm(A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
